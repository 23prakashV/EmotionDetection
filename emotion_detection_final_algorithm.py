# -*- coding: utf-8 -*-
"""Emotion Detection Final Algorithm

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tUfF0XyizMwLkuKFueGT04S_EVuVFycd
"""

import cv2
import dlib
import pickle
import warnings
import numpy as np
import pandas as pd
import seaborn as sns

import urllib.request

from sklearn import metrics
from scipy.spatial import distance
from sklearn.metrics import accuracy_score
from matplotlib import pyplot as plt
from tqdm import tqdm,tqdm_pandas
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

import re
import gdown
import keras

from keras.models import Sequential
from keras.utils import to_categorical
from keras.optimizers import Adam
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from keras.losses import categorical_crossentropy
from keras.optimizers import Adam, SGD
from keras.regularizers import l2
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import load_model

# grab tools from our tensorflow and keras toolboxes!
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import optimizers

warnings.filterwarnings("ignore")
def model_to_string(model):
    import re
    stringlist = []
    model.summary(print_fn=lambda x: stringlist.append(x))
    sms = "\n".join(stringlist)
    sms = re.sub('_\d\d\d','', sms)
    sms = re.sub('_\d\d','', sms)
    sms = re.sub('_\d','', sms)  
    return sms

!wget -O ./ferdata.csv https://www.dropbox.com/s/rwb5hepfn1uw212/fer2013_5.csv?dl=1
dataset_path = './ferdata.csv'

!wget -O ./shape_predictor_68_face_landmarks.dat https://www.dropbox.com/s/ch5euco240mvjgq/shape_predictor_68_face_landmarks.dat?dl=1
dlibshape_path ='./shape_predictor_68_face_landmarks.dat'

!wget -O ./pureX.npy https://www.dropbox.com/s/6kmzfluo4hn5795/pureX.npy?dl=1
pureX_path = './pureX.npy'

!wget -O ./dataX.npy https://www.dropbox.com/s/uid9ndffnzzpuqn/dataX.npy?dl=1
dataX_path = './dataX.npy'

!wget -O ./dataY.npy https://www.dropbox.com/s/2p6vm3kfrik5lzz/dataY.npy?dl=1
dataY_path = './dataY.npy'

print ("Data Downloaded")


def plot_confusion_matrix(y_true,y_predicted):
  cm = metrics.confusion_matrix(y_true, y_predicted)
  print ("Plotting the Confusion Matrix")
  labels = list(label_map.values())
  df_cm = pd.DataFrame(cm,index = labels,columns = labels)
  fig = plt.figure()
  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')
  plt.yticks([0.5,1.5,2.5,3.5,4.5], labels,va='center')
  plt.title('Confusion Matrix - TestData')
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
 
  plt.show()
  plt.close()

def plot_graphs(history, best):
  
  plt.figure(figsize=[10,4])
  # summarize history for accuracy
  plt.subplot(121)
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title('model accuracy across training\n best accuracy of %.02f'%best[1])
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  
  # summarize history for loss
  plt.subplot(122)
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('model loss across training\n best loss of %.02f'%best[0])
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()


label_map = {"0":"ANGRY","1":"HAPPY","2":"SAD","3":"SURPRISE","4":"NEUTRAL"}


predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')


def get_landmarks(image): 
  rects = [dlib.rectangle(left=1, top=1, right=47, bottom=47)]
  image = cv2.imread(image_path)
  landmarks = [(p.x, p.y) for p in predictor(image, rects[0]).parts()]
  return image,landmarks

#defining 68 landmarks
def image_landmarks(image,face_landmarks):
  radius = -4
  circle_thickness = 1
  image_copy = image.copy()
  for (x, y) in face_landmarks:
    cv2.circle(image_copy, (x, y), circle_thickness, (255,0,0), radius)
    
  plt.imshow(image_copy, interpolation='nearest')
  plt.show()
  

#calculates euclidian distances between landmarks  
def landmarks_edist(face_landmarks):
    e_dist = []
    for i in range(len(face_landmarks)):
        for j in range(len(face_landmarks)):
            if i!= j:
                e_dist.append(distance.euclidean(face_landmarks[i],face_landmarks[j]))
    return e_dist
  
def compare_learning(): 


  #2 Layer MLP Model
  tmp_model = Sequential()
  tmp_model.add(Dense(7, input_shape=(5,), activation = 'relu')) 
  tmp_model.add(Dense(7, activation = 'relu'))
  tmp_model.add(Dense(4, activation = 'linear'))

  tmp_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])

  #Neural Network Model
  perceptron = Sequential()
  perceptron.add(Dense(units = 1024, input_shape = (4556,),kernel_initializer='glorot_normal',activation = 'relu'))
  perceptron.add(Dense(units = 512,kernel_initializer='glorot_normal' , activation = 'relu'))
  perceptron.add(Dense(units = 5, activation = 'softmax'))
  perceptron.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), metrics=['accuracy'])
  perceptron_answer = Sequential()
  perceptron_answer.add(Dense(units = 1024, input_shape = (4556,),kernel_initializer='glorot_normal',activation = 'relu'))
  perceptron_answer.add(Dense(units = 512,kernel_initializer='glorot_normal' , activation = 'relu'))
  perceptron_answer.add(Dense(units = 5, activation = 'softmax'))
  perceptron_answer.compile(
    loss='categorical_crossentropy',
    optimizer=SGD(lr=0.001),
    metrics=['accuracy'])
  if model_to_string(perceptron) == model_to_string(perceptron_answer):
    print('Model rendered sucessfully')
  else: 
    print('Failure to render')

  # hyperparameters
  epochs = 20
  batch_size = 64
  test_ratio = .1
  n_labels = 5 

  # load data 
  dataX_pixels = np.load('pureX.npy')
  dataY_pixels = np.load('dataY.npy')

  # hot encoding
  y_onehot = keras.utils.to_categorical(dataY_pixels, len(set(dataY_pixels)))

  # split and standardize data
  X_train, X_test, y_train, y_test = train_test_split(dataX_pixels, y_onehot, test_size=test_ratio, random_state=42)

  pixel_scaler = StandardScaler()
  pixel_scaler.fit(X_train)
  X_train = pixel_scaler.transform(X_train)
  X_test = pixel_scaler.transform(X_test)

  # MLP Model (with more layers)
  mlp_model = Sequential()
  mlp_model.add(Dense(5120, activation='relu',kernel_initializer='glorot_normal', input_shape=( X_train.shape[1]   ,)))
  mlp_model.add(Dropout(0.5))
  mlp_model.add(Dense(512,kernel_initializer='glorot_normal', activation='relu'))
  mlp_model.add(Dropout(0.5))
  mlp_model.add(Dense(256,kernel_initializer='glorot_normal', activation='relu'))
  mlp_model.add(Dropout(0.5))
  mlp_model.add(Dense(n_labels, activation='softmax'))
  mlp_model.summary()
  mlp_model.compile(loss=categorical_crossentropy, optimizer=SGD(lr=0.001), metrics=['accuracy'])
  checkpoint = ModelCheckpoint('best_mlp_model.h5', verbose=1, monitor='val_accuracy', save_best_only=True,  mode='auto') 
  mlp_history = mlp_model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, 
                              callbacks=[checkpoint], validation_data=(X_test, y_test), shuffle=True)
  mlp_performance = mlp_model.evaluate(X_test, y_test, batch_size=64)
  plot_graphs(mlp_history, mlp_performance); 
  y_pred_mlp = mlp_model.predict_classes(X_test)
  y_true = np.argmax(y_test,axis=1)
  plot_confusion_matrix(y_true, y_pred_mlp)


  # CNN Model
  width, height = 48, 48
  print(X_train.shape) 
  X_train_cnn = X_train.reshape(len(X_train),height,width)
  X_test_cnn = X_test.reshape(len(X_test),height,width)

  print(X_train_cnn.shape) 
  print(X_test_cnn.shape) 

  cnn_model = Sequential()

  cnn_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(width, height, 1)))
  cnn_model.add(BatchNormalization())
  cnn_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
  cnn_model.add(Dropout(0.5))
  cnn_model.add(Flatten())
  cnn_model.add(Dense(512, activation='relu'))
  cnn_model.add(Dense(n_labels, activation='softmax'))
  checkpoint = ModelCheckpoint('best_cnn_model.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  
  cnn_model.compile(loss=categorical_crossentropy, optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999), metrics=['accuracy'])
  cnn_history = cnn_model.fit(X_train_cnn, y_train, batch_size=batch_size, epochs=epochs, verbose=1, 
                              callbacks=[checkpoint], validation_data=(X_test_cnn, y_test), shuffle=True)
  cnn_performance = cnn_model.evaluate(X_test_cnn, y_test, batch_size=64)
  plot_graphs(cnn_history, cnn_performance)


compare_learning()